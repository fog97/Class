{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll use many classification models on a dataset about Kickstarter projects available on Kaggle following this [link](https://www.kaggle.com/kemical/kickstarter-projects). \r\n",
    "I wasn't able to upload data on github due to their dimensions.  \r\n",
    "In this code I'm going to use the [Sci-kit Learn Package](https://scikit-learn.org/stable/): I'm going to import each instance or method in the cell where I use it in order to clarify their use.  \r\n",
    "However, I won't comment model performances: I'll just give some information about how the AUC index and the ROC Curves are interpreted.  \r\n",
    "This is a short guide which can be useful to quickly perform a classification task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to prepare data I'm going to use a self-created class. I uploaded this object on [Github](https://github.com/fog97/Class).\r\n",
    "I won't use a particular preprocessing for each model: I'm just going to prepare data and use them as trainig and testing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, I delete many levels from the target variable in order to have a binary classification task.  \r\n",
    "I limit the numer of rows too, because there are too many for my machine resources, hence I'm going to work on 25000 rows.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "d=data_preparation()\r\n",
    "data=d.importer('./kickstarter.csv','r',',')\r\n",
    "#eimino le classi del target collegate a stati temporanei\r\n",
    "#restano 2 classi\r\n",
    "stop=[]\r\n",
    "for el in data.state:\r\n",
    "  if el!='failed' and el!='successful' and el not in stop:\r\n",
    "    stop.append(el)\r\n",
    "for parola in stop:    \r\n",
    "  data=data[data.state!=parola]\r\n",
    "data1=data.iloc[:25000,]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I'm going to impute missing: if missingness are present, discretize categorical variables and recoding the target variable form \"successfull\" \"failed\" to 0,1 respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "colname=list(data1.columns)\r\n",
    "Y=data1.state\r\n",
    "num=data1.select_dtypes(include=('float','int'))\r\n",
    "char=data1.drop(list(num),axis=1).drop('state',axis=1)\r\n",
    "cat_clean=char.drop(['ID','name','deadline','launched','category'],axis=1)\r\n",
    "lab=[\"Var\",\"Var2\",\"Chi_quadro\",\"P_Value\"]\r\n",
    "numm=d.num_imputation(num)\r\n",
    "cat=d.cat_imputation(cat_clean)\r\n",
    "catt=d.cat_encoding(cat)\r\n",
    "\r\n",
    "num_clean=np.delete(numm,[1,3,4,7,8],axis=1)\r\n",
    "f=0\r\n",
    "s=0\r\n",
    "for st in Y:\r\n",
    "  if st=='failed':\r\n",
    "    f+=1\r\n",
    "  else:\r\n",
    "    s+=1\r\n",
    "prior_f=round(f/len(data1[1:]),3)\r\n",
    "prior_s=round(s/len(data1[1:]),3)\r\n",
    "#1=Failed, 0=Successful\r\n",
    "Yy=np.empty(1)\r\n",
    "for el in Y:\r\n",
    "  if el=='failed':\r\n",
    "    Yy=np.append(Yy,1)\r\n",
    "  else:\r\n",
    "    Yy=np.append(Yy,0)\r\n",
    "Yy=np.delete(Yy,0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last operation on data is the creation of train and test samples.  \r\n",
    "Using 60%-40% percentages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "df=np.append(num_clean,catt,axis=1)\r\n",
    "X=df\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Yy, test_size=0.6, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I'm ready to start with the first model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naive Bayes algorithm is a way to approximate the not existing but theorically Bayesian Classifier.  \n",
    "I order to do this the algorithm uses the bayesian theorem, but assuming independence between all variables used as explanatory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "gnb = GaussianNB(priors=(prior_f,prior_s),  var_smoothing=1e-30)\r\n",
    "NB_pred = gnb.fit(X_train, Y_train).predict(X_test)\r\n",
    "print(\"Naive Bayes:Number of mislabeled points out of a total %d points : %d\"\r\n",
    "  % (X_test.shape[0], (Y_test != NB_pred).sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Decision Tree is a powerful algorthm: it divides data from the full dataset into pure nodes. It doesn't need preprocessing, I'll use preprocessed data in order to write a shorter code.  \r\n",
    "The only weakness of this kind of algorithm is the high instability of the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import tree\r\n",
    "DecTree = tree.DecisionTreeClassifier()\r\n",
    "Tree_pred = DecTree.fit(X_train, Y_train).predict(X_test)\r\n",
    "print(\"Tree:Number of mislabeled points out of a total %d points : %d\"\r\n",
    "  % (X_test.shape[0], (Y_test != Tree_pred).sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bagging is a method that consist in using multiple algorithms of the same kind on different data samples created by bootstrap.  \r\n",
    "In this case I'm using bagging on both **Naive Bayes**, the first code block, and **Decision Tree**. This tecnique is used to improve tree's performances by reducing its instability."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Naive Bayes Bagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import BaggingClassifier\r\n",
    "Bagging = BaggingClassifier(gnb,max_samples=0.5, max_features=0.5)\r\n",
    "Bagging_pred = Bagging.fit(X_train, Y_train).predict(X_test)\r\n",
    "print(\"Bagging:Number of mislabeled points out of a total %d points : %d\"\r\n",
    "  % (X_test.shape[0], (Y_test != Bagging_pred).sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision Tree Bagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import BaggingClassifier\r\n",
    "Bagging_Tree = BaggingClassifier(DecTree,max_samples=0.5, max_features=0.6)\r\n",
    "Bagging_Tree_pred = Bagging_Tree.fit(X_train, Y_train).predict(X_test)\r\n",
    "print(\"Tree Baging :Number of mislabeled points out of a total %d points : %d\"\r\n",
    "  % (X_test.shape[0], (Y_test != Bagging_Tree_pred).sum()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Support Vector Machines are a family of algorithms very powerful but with difficult interpretation.  \r\n",
    "This algorithm uses, in order to classify, an hyperplane. It divides the space in which observetion are represented and classifies using the distance between each observation and the hyperplan. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import svm\r\n",
    "SVM = svm.SVC()\r\n",
    "SVM_pred = SVM.fit(X_train, Y_train).predict(X_test)\r\n",
    "print(\"SVM:Number of mislabeled points out of a total %d points : %d\"\r\n",
    "  % (X_test.shape[0], (Y_test != SVM_pred).sum()))\r\n",
    "print('Traccio curve Roc e calcolo AUC.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROC Curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ROC curve is a graphic in which are represented the True Positive Rate and the False Positive Rate for all the possible probabilistic thresholds, using this graphic you can understand if a model is always the best one or if it can be improved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import plot_roc_curve\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "ax = plt.gca()\r\n",
    "NB_disp = plot_roc_curve(gnb, X_test, Y_test,ax=ax,alpha=10)\r\n",
    "Tree_disp = plot_roc_curve(DecTree,X_test, Y_test, ax=ax, alpha=10)\r\n",
    "Bagging_Tree_disp = plot_roc_curve(Bagging_Tree, X_test, Y_test,ax=ax,alpha=10)\r\n",
    "Bagging_disp = plot_roc_curve(Bagging, X_test, Y_test,ax=ax,alpha=10)\r\n",
    "SVM_disp = plot_roc_curve(SVM, X_test, Y_test,ax=ax,alpha=10)\r\n",
    "plt.show()\r\n",
    "print('Precision recall Curve')\r\n",
    "from sklearn.metrics import precision_recall_curve\r\n",
    "from sklearn.metrics import plot_precision_recall_curve\r\n",
    "disp = plot_precision_recall_curve(Bagging_Tree, X_test, Y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually a good classification model has an AUC(Area Under the Curve, calculated as the area under the ROC curve) higher than 0.80. Graphically, a good AUC is represented if the curve is a lot higher than the bisector of the quadrant.  "
   ],
   "metadata": {}
  }
 ]
}